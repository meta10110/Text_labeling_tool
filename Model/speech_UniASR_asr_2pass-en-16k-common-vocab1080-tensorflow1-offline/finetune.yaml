# hybrid CTC/attention
model_conf:
    ctc_weight: 0.0
    lsm_weight: 0.1     # label smoothing option
    length_normalized_loss: true
    predictor_weight: 1.0
    decoder_attention_chunk_type: chunk
    ctc_weight2: 0.0
    predictor_weight2: 1.0
    decoder_attention_chunk_type2: chunk
    loss_weight_model1: 0.5
    enable_maas_finetune: true

# minibatch related
# dataset_type: small
batch_type: length
batch_bins: 2000
num_workers: 16
speech_length_min: 100
speech_length_max: 15000
#dataset_type: large
dataset_conf:
    data_types: sound,text
    filter_conf:
        speech_length_min: 100
        speech_length_max: 15000
        token_length_min: 0
        token_length_max: 200
    shuffle: True
    shuffle_conf:
        shuffle_size: 2048
        sort_size: 500
    batch_conf:
        batch_type: token
        batch_size: 120000
    num_workers: 16

# optimization related
accum_grad: 1
grad_clip: 5
max_epoch: 20
val_scheduler_criterion:
    - valid
    - acc
best_model_criterion:
-   - valid
    - acc
    - max
keep_nbest_models: 20

optim: adam
optim_conf:
   lr: 0.0001
scheduler: warmuplr
scheduler_conf:
   warmup_steps: 30000

specaug: specaug_lfr
specaug_conf:
    apply_time_warp: false
    time_warp_window: 5
    time_warp_mode: bicubic
    apply_freq_mask: true
    freq_mask_width_range:
    - 0
    - 30
    lfr_rate: 6
    num_freq_mask: 1
    apply_time_mask: true
    time_mask_width_range:
    - 0
    - 12
    num_time_mask: 1


log_interval: 50
normalize: None
split_with_space: true
unused_parameters: true
